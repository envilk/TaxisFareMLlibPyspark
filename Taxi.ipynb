{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark - Taxi Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported several libraries according to what we needed to build the models up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark as pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use from Spark the API's for Machine Learning and DataBases purposes, so by Native Spark we mean to use that libraries without using any other component of Spark for doing this example. It's important to take it in a count because later we are going to use clustering for solve the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a Spark Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is for create an interface between the Jupyter Notebook and the Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set with the spark method to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charging the LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to see the type of the attributes to check if we need to process some categorical attributes, but in this case we don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'string'),\n",
       " ('fare_amount', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('pickup_longitude', 'string'),\n",
       " ('pickup_latitude', 'string'),\n",
       " ('dropoff_longitude', 'string'),\n",
       " ('dropoff_latitude', 'string'),\n",
       " ('passenger_count', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's necessary to cast all the attributes to float because it's the one of the datatypes that Spark works with, not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to select the most imporant features to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if everything it's okay just taking a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to gather all the features into one colum called \"features\" because Spark needs it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55423856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to delete null rows because Spark doesn't accept null ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|            features|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|[-73.844314575195...|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|[-74.016044616699...|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|[-73.982734680175...|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|[-73.987129211425...|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|[-73.968093872070...|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|[-74.000961303710...|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|[-73.980003356933...|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|[-73.951301574707...|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|[-74.006462097167...|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|[-73.980659484863...|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|[-73.996337890625...|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|       (5,[4],[3.0])|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|[-73.991600036621...|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|[-74.005363464355...|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|[-74.001823425292...|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|       (5,[4],[1.0])|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|[-73.986427307128...|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|[-73.981056213378...|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|[-73.969505310058...|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|[-73.979812622070...|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete null rows\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Linear Regression algorithms for train the model. We also have to measue the time that the model spends to train the data, which is one of the main purposes of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:08:17.698225\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(new_df.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically here we show how's the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 20.710237\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before but for Decision tree regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) 0:20:05.413627\n",
      "+------------------+------+--------------------+\n",
      "|        prediction| label|            features|\n",
      "+------------------+------+--------------------+\n",
      "| 34.02801889571103|-29.87|[-73.863159179687...|\n",
      "|11.850275013996376| -20.0|       (5,[4],[5.0])|\n",
      "| 8.896463517049114|  -6.5|[-73.984352111816...|\n",
      "| 8.896463517049114|  -6.0|[-73.987518310546...|\n",
      "| 8.896463517049114|  -3.0|[-73.995063781738...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 23.9768\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before but for Random Forest regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) 0:35:53.671012\n",
      "+------------------+------+--------------------+\n",
      "|        prediction| label|            features|\n",
      "+------------------+------+--------------------+\n",
      "|18.195694139170584| -44.9|[-73.871116638183...|\n",
      "|30.771499981119245|-29.87|[-73.863159179687...|\n",
      "|13.290360749502483| -18.1|[-73.958274841308...|\n",
      "| 8.933319739165913| -10.1|[-73.983306884765...|\n",
      "| 8.933319739165913|  -6.5|[-73.984352111816...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 24.8237\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Train model.  \n",
    "start_time = datetime.now()\n",
    "\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Local Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark implicitly split his own data structures to parallelize them into the clusters, so we set a master for manage the others slaves and then we run as many slaves as logical cores has our own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local[*] string is a special string denoting that you’re using a local cluster,\n",
    "which is another way of saying you’re running in single-machine mode. \n",
    "The * tells Spark to create as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before, to prepare the dataset to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Local Cluster Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same kind of training, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:07:36.215578\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(df_local_clust.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures\n",
    "\n",
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing again the RMSE for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 20.710237\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression local cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before but for Decision tree regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:13:05.248696\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|  8.93128770001487|-45.0|[-73.980125427246...|\n",
      "|13.792550010457832|-18.1|[-73.958274841308...|\n",
      "|  8.93128770001487|-10.1|[-73.983306884765...|\n",
      "|  8.93128770001487| -6.5|[-73.984352111816...|\n",
      "|  8.93128770001487| -5.3|[-73.984802246093...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 24.0373\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression local cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before but for Random Forest regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:26:28.543485\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 8.887903540270049|-45.0|[-73.980125427246...|\n",
      "|13.359459177697568|-18.1|[-73.958274841308...|\n",
      "| 8.887903540270049|-10.1|[-73.983306884765...|\n",
      "|11.238500888096038| -4.5|[-74.006141662597...|\n",
      "| 8.934682006302216| -3.0|[-74.004997253417...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 6.8643\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can appreciate the Root Mean Squared Error is the same because we use the same algorithm, but if we check the time is 58% faster working with the cluster, even if the are local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
