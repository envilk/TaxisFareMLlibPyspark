{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark - Taxi Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark as pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use from Spark the API's for Machine Learning and DataBases purposes, so by Native Spark we mean to use that libraries without using any other component of Spark for doing this example. It's important to take it in a count because later we are going to use clustering for solve the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a Spark Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is for create an interface between the Jupyter Notebook and the Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'string'),\n",
       " ('fare_amount', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('pickup_longitude', 'string'),\n",
       " ('pickup_latitude', 'string'),\n",
       " ('dropoff_longitude', 'string'),\n",
       " ('dropoff_latitude', 'string'),\n",
       " ('passenger_count', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to select the most imporant features to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55423856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|            features|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|[-73.844314575195...|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|[-74.016044616699...|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|[-73.982734680175...|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|[-73.987129211425...|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|[-73.968093872070...|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|[-74.000961303710...|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|[-73.980003356933...|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|[-73.951301574707...|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|[-74.006462097167...|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|[-73.980659484863...|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|[-73.996337890625...|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|       (5,[4],[3.0])|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|[-73.991600036621...|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|[-74.005363464355...|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|[-74.001823425292...|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|       (5,[4],[1.0])|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|[-73.986427307128...|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|[-73.981056213378...|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|[-73.969505310058...|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|[-73.979812622070...|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete null rows\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Linear Regression algorithms for train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:11:17.159047\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(new_df.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 11.345004953120542\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "RMSE: 20.710867\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) 0:20:22.153943\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 8.923282596071983|-62.0|[-74.002838134765...|\n",
      "| 33.94856839028241|-44.9|[-73.871116638183...|\n",
      "|13.820852104281952|-18.1|[-73.958274841308...|\n",
      "| 8.923282596071983|-10.1|[-73.983306884765...|\n",
      "| 8.923282596071983| -6.5|[-73.984352111816...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 28.3057\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) 0:35:37.276246\n",
      "+------------------+------+--------------------+\n",
      "|        prediction| label|            features|\n",
      "+------------------+------+--------------------+\n",
      "|28.582299205178288|-29.87|[-73.863159179687...|\n",
      "|14.533684751999676| -18.1|[-73.958274841308...|\n",
      "|  8.88273460890856|  -6.5|[-73.993659973144...|\n",
      "|  8.88273460890856|  -6.5|[-73.989494323730...|\n",
      "|  8.88273460890856|  -5.3|[-73.984802246093...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 23.9302\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Train model.  \n",
    "start_time = datetime.now()\n",
    "\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Local Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark implicitly split his own data structures to parallelize them into the clusters, so we set a master for manage the others slaves and then we run as many slaves as logical cores has our own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local[*] string is a special string denoting that you’re using a local cluster,\n",
    "which is another way of saying you’re running in single-machine mode. \n",
    "The * tells Spark to create as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library for work with Clusters\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before, for preparing the dataset to be train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "df_local_clust = vecAssembler.transform(df)\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'passenger_count',\n",
       " 'features']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_local_clust.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Local Cluster Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:07:40.082209\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(df_local_clust.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 11.345004953120545\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures\n",
    "\n",
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "RMSE: 20.710867\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:11:35.108962\n",
      "+-----------------+-----+--------------------+\n",
      "|       prediction|label|            features|\n",
      "+-----------------+-----+--------------------+\n",
      "|8.742626065536477|-62.0|[-74.002838134765...|\n",
      "|8.742626065536477|-10.1|[-73.983306884765...|\n",
      "|8.742626065536477| -6.5|[-73.981338500976...|\n",
      "|8.742626065536477| -6.0|[-73.987518310546...|\n",
      "|8.742626065536477| -5.0|[-73.990974426269...|\n",
      "+-----------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 6.96171\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:18:30.459010\n",
      "+-----------------+------+--------------------+\n",
      "|       prediction| label|            features|\n",
      "+-----------------+------+--------------------+\n",
      "| 8.96474941906633| -62.0|[-74.002838134765...|\n",
      "|31.47212434781009|-29.87|[-73.863159179687...|\n",
      "|12.33413473421554| -18.1|[-73.958274841308...|\n",
      "| 8.96474941906633| -10.1|[-73.983306884765...|\n",
      "| 8.96474941906633|  -5.0|[-73.990974426269...|\n",
      "+-----------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 23.9699\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can appreciate the Root Mean Squared Error is the same because we use the same algorithm, but if we check the time is 58% faster working with the cluster, even if the are local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
