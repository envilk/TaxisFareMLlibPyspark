{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark - Taxi Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark as pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use from Spark the API's for Machine Learning and DataBases purposes, so by Native Spark we mean to use that libraries without using any other component of Spark for doing this example. It's important to take it in a count because later we are going to use clustering for solve the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a Spark Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is for create an interface between the Jupyter Notebook and the Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'string'),\n",
       " ('fare_amount', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('pickup_longitude', 'string'),\n",
       " ('pickup_latitude', 'string'),\n",
       " ('dropoff_longitude', 'string'),\n",
       " ('dropoff_latitude', 'string'),\n",
       " ('passenger_count', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to select the most imporant features to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55423856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|label|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|            features|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "|  4.5|      -73.844315|      40.721317|        -73.84161|       40.712276|            1.0|[-73.844314575195...|\n",
      "| 16.9|      -74.016045|      40.711304|        -73.97927|       40.782005|            1.0|[-74.016044616699...|\n",
      "|  5.7|      -73.982735|       40.76127|        -73.99124|        40.75056|            2.0|[-73.982734680175...|\n",
      "|  7.7|       -73.98713|      40.733143|        -73.99157|        40.75809|            1.0|[-73.987129211425...|\n",
      "|  5.3|      -73.968094|       40.76801|        -73.95666|       40.783764|            1.0|[-73.968093872070...|\n",
      "| 12.1|       -74.00096|       40.73163|        -73.97289|       40.758232|            1.0|[-74.000961303710...|\n",
      "|  7.5|          -73.98|      40.751663|         -73.9738|       40.764843|            1.0|[-73.980003356933...|\n",
      "| 16.5|        -73.9513|       40.77414|         -73.9901|        40.75105|            1.0|[-73.951301574707...|\n",
      "|  9.0|       -74.00646|       40.72671|        -73.99308|        40.73163|            1.0|[-74.006462097167...|\n",
      "|  8.9|       -73.98066|       40.73387|        -73.99154|       40.758137|            2.0|[-73.980659484863...|\n",
      "|  5.3|       -73.99634|       40.73714|        -73.98072|        40.73356|            1.0|[-73.996337890625...|\n",
      "|  5.5|             0.0|            0.0|              0.0|             0.0|            3.0|       (5,[4],[3.0])|\n",
      "|  4.1|        -73.9916|      40.744713|        -73.98308|       40.744682|            2.0|[-73.991600036621...|\n",
      "|  7.0|       -74.00536|      40.728867|        -74.00891|       40.710907|            1.0|[-74.005363464355...|\n",
      "|  7.7|       -74.00182|       40.73755|        -73.99806|        40.72279|            2.0|[-74.001823425292...|\n",
      "|  5.0|             0.0|            0.0|              0.0|             0.0|            1.0|       (5,[4],[1.0])|\n",
      "| 12.5|       -73.98643|      40.760464|        -73.98899|       40.737076|            1.0|[-73.986427307128...|\n",
      "|  5.3|       -73.98106|       40.73769|        -73.99418|       40.728413|            1.0|[-73.981056213378...|\n",
      "|  5.3|      -73.969505|      40.784843|        -73.95873|       40.783356|            1.0|[-73.969505310058...|\n",
      "|  4.0|       -73.97981|      40.751904|        -73.97945|        40.75548|            1.0|[-73.979812622070...|\n",
      "+-----+----------------+---------------+-----------------+----------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete null rows\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Linear Regression algorithms for train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:04:45.396320\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(new_df.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 11.345004953120545\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "RMSE: 20.710867\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Local Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark implicitly split his own data structures to parallelize them into the clusters, so we set a master for manage the others slaves and then we run as many slaves as logical cores has our own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local[*] string is a special string denoting that you’re using a local cluster,\n",
    "which is another way of saying you’re running in single-machine mode. \n",
    "The * tells Spark to create as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library for work with Clusters\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as before, for preparing the dataset to be train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.csv('/home/goldenfox//Escritorio/ApacheSpark/TaxisFareMLlibPyspark-master/data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "df_local_clust = vecAssembler.transform(df)\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'passenger_count',\n",
       " 'features']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_local_clust.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Local Cluster Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:02:56.655427\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(df_local_clust.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 11.345004953120545\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures\n",
    "\n",
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "RMSE: 20.710867\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can appreciate the Root Mean Squared Error is the same because we use the same algorithm, but if we check the time is 58% faster working with the cluster, even if the are local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
