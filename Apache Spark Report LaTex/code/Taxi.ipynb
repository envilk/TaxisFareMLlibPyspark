{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark - Taxi Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries imported according to what is needed to build up the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark as pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs for Machine Learning and DataBases purposes will be used. Native Clustering means using libraries that implicitly parallelize the data and fault tolerance. It's important to take it into account because later it's used  to solve the same problem but with another way of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a Spark Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this method is to create an interface between the Jupyter Notebook and the Spark-shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set with the spark method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's crutial to see the type of the attributes to check if it's needed to process some categorical attributes, but not in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'string'),\n",
       " ('fare_amount', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('pickup_longitude', 'string'),\n",
       " ('pickup_latitude', 'string'),\n",
       " ('dropoff_longitude', 'string'),\n",
       " ('dropoff_latitude', 'string'),\n",
       " ('passenger_count', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's necessary to cast all the attributes to float because it's one of the datatypes that Spark works with, because Spark doesn't work with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of the most important features to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the correctness of the features, by taking a look at the current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering all the features into one colum called \"features\" because Spark needs to work with all the features in one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55423856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's vital to delete null rows because Spark doesn't accept them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Delete null rows\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Linear Regression algorithms to train the model. It's necessary to measure the time the model spends to train the data, which is one of the main purposes of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:08:17.698225\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(new_df.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 20.710237\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Decision Tree with Local Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) 0:20:05.413627\n",
      "+------------------+------+--------------------+\n",
      "|        prediction| label|            features|\n",
      "+------------------+------+--------------------+\n",
      "| 34.02801889571103|-29.87|[-73.863159179687...|\n",
      "|11.850275013996376| -20.0|       (5,[4],[5.0])|\n",
      "| 8.896463517049114|  -6.5|[-73.984352111816...|\n",
      "| 8.896463517049114|  -6.0|[-73.987518310546...|\n",
      "| 8.896463517049114|  -3.0|[-73.995063781738...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 23.9768\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Random Forest Regression with Local Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) 0:35:53.671012\n",
      "+------------------+------+--------------------+\n",
      "|        prediction| label|            features|\n",
      "+------------------+------+--------------------+\n",
      "|18.195694139170584| -44.9|[-73.871116638183...|\n",
      "|30.771499981119245|-29.87|[-73.863159179687...|\n",
      "|13.290360749502483| -18.1|[-73.958274841308...|\n",
      "| 8.933319739165913| -10.1|[-73.983306884765...|\n",
      "| 8.933319739165913|  -6.5|[-73.984352111816...|\n",
      "+------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 24.8237\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('taxis_fare').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/train.csv\")\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "new_df = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Train model.  \n",
    "start_time = datetime.now()\n",
    "\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Local Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark implicitly split his own data structures to parallelize them into the clusters, so we set a master to manage the others slaves and then we run as many slaves as logical cores our own computer has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local[*] string is a special string that denots you’re using a local cluster,\n",
    "which is another way of saying you’re running in single-machine mode with several cores working as clusters. \n",
    "The * tells Spark to create as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset as before to be trained in a local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) 0:07:36.215578\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "lrModel = lr.fit(df_local_clust.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF LINEAR REGRESSION TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures\n",
    "\n",
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 20.710237\n"
     ]
    }
   ],
   "source": [
    "#Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:13:05.248696\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|  8.93128770001487|-45.0|[-73.980125427246...|\n",
      "|13.792550010457832|-18.1|[-73.958274841308...|\n",
      "|  8.93128770001487|-10.1|[-73.983306884765...|\n",
      "|  8.93128770001487| -6.5|[-73.984352111816...|\n",
      "|  8.93128770001487| -5.3|[-73.984802246093...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 24.0373\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = dt.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF DECISION TREE ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) 0:26:28.543485\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 8.887903540270049|-45.0|[-73.980125427246...|\n",
      "|13.359459177697568|-18.1|[-73.958274841308...|\n",
      "| 8.887903540270049|-10.1|[-73.983306884765...|\n",
      "|11.238500888096038| -4.5|[-74.006141662597...|\n",
      "| 8.934682006302216| -3.0|[-74.004997253417...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 6.8643\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('taxi_fare_local_cluster').config('spark.driver.memory', '8g').getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv('data/train.csv', header=True,\n",
    "                    inferSchema=True, nullValue=' ')\n",
    "\n",
    "df = df.select(df['fare_amount'].cast(\"float\").alias('fare_amount'),\n",
    "               df['pickup_longitude'].cast(\"float\").alias('pickup_longitude'),\n",
    "               df['pickup_latitude'].cast(\"float\").alias('pickup_latitude'),\n",
    "               df['dropoff_longitude'].cast(\"float\").alias('dropoff_longitude'),\n",
    "               df['dropoff_latitude'].cast(\"float\").alias('dropoff_latitude'),\n",
    "               df['passenger_count'].cast(\"float\").alias('passenger_count'))\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"fare_amount as label\",'pickup_longitude','pickup_latitude',\n",
    "                  'dropoff_longitude','dropoff_latitude','passenger_count')\n",
    "\n",
    "#colum features\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_longitude\", \"pickup_latitude\",\n",
    "                                          \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                                          \"passenger_count\"], outputCol=\"features\")\n",
    "\n",
    "#Delete null rows\n",
    "df_local_clust = vecAssembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_local_clust.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(trainingData.select('label','features'))\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "print('TIME OF RANDOM FOREST ON LOCAL CLUSTER TRAINING (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe the Root Mean Squared Error is the same because we use the same algorithm in the same dataset, something relevant to be able to compare them with different approaches. As we see the time is 58% faster while working with the cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
