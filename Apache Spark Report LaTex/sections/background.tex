\section{The Software Technology}
\label{sec:background}

Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. It is based on a data computing and analytics system based on Hadoop Map Reduce.

The official web defines Apache Spark as an unified analytics engine for large-scale data processing.

Apache Spark has as its architectural foundation the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.\cite{brown:zaharia2010spark} The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged \cite{brown:eadline2015hadoop} even though the RDD API is not deprecated The RDD technology still underlies the Dataset API.
It works in memory, which results in a much faster processing speed, it allows you to work on disk with large amount of data. In this way if for example we have a very large file or a quantity of information that does not fit in memory, the tool allows to store part in disk, which makes lose speed. This means that we have to try to find the balance between what is stored in memory and what is stored in disk, to have a good speed and so that the cost is not too high, as the memory is always much more expensive than the disk.

Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.\cite{zaharia2012resilient}

Spark facilitates the implementation of both iterative algorithms, which visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by several orders of magnitude compared to Apache Hadoop MapReduce implementation Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark

Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster, where you can launch a cluster either manually or use the launch scripts provided by the install package. It is also possible to run these daemons on a single machine for testing), Hadoop YARN, Apache Mesos or Kubernetes. For distributed storage, Spark can interface with a wide variety, including Alluxio, Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, Lustre file system, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\cite{wang2014characterization}

The friendly APIs ,which Apache Spark provides, natively supports Java, Scala, R, and Python, giving you a variety of languages for building your applications. These APIs make it easy for developers, because they hide the complexity of distributed processing behind simple, high-level operators that dramatically lowers the amount of code required.
It allows real time processing, with a module called Spark Streaming, which combined with Spark SQL will allow us to process the data in real time. As we are injecting the data we can transform them and turn them to a final result.
Resilient Distributed Dataset (RDD): It uses the lazy evaluation, which means that all the transformations that we are carrying out on the RDD, are not resolved, but are stored in a directed acyclic graph (DAG), and when we execute an action, that is to say, when the tool does not have more option than to execute all the transformations, it will be when they are executed. This is a double-edged sword, as it has an advantage and a disadvantage. The advantage is that you gain speed by not making transformations continuously, but only when necessary. The disadvantage is that if some transformation raises some kind of exception, it will not be detected until the action is executed, so it is more difficult to debug or program.

\subsection{Spark Architecture}

The main components that make up the framework are the following ones:

\begin{description}

	\item[Spark Core]: It is the base or set of libraries where the rest of modules are supported. It is the core of the framework.
	
	
	\item[Spark SQL]: It is the module for the processing of structured and semi-structured data. With this module we will be able to transform and perform operations on RDD or dataframes. It is designed exclusively for data processing.
	
	
	\item[Spark Streaming]: It is the one that allows the ingestion of data in real time. If we have a source, for example Kafka or Twitter, with this module we can ingest data from that source and dump them to a destination. Between the ingestion of data and its subsequent dump, we can have a series of transformations.
	
	
	\item[Spark MLLib]: It is a very complete library that contains numerous Machine Learning algorithms, both clustering, classification, regression, and so on. It allows us, in a friendly way, to use Machine Learning algorithms.
	
	
	\item[Spark Graph]: Allows the processing of graphs (DAG). It does not allow to paint graphs, but it allows to create operations with graphs, with their nodes and edges, and to go making operations.
\end{description}


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.65]{figs/spark-components.png}
	\caption{Components of Apache Spark}
	\label{fig:components-spark}
\end{figure}

\subsection{MLlib}

MLIB is part of Spark APIs and is interoperable with Python NumPy as well as the R libraries. Implemented with Spark it is possible to use any type of data from any source or from the Hadoop platform such as HDFS, HBase, related database data sources or local data sources such as text documents.\\*

Spark excels in iterative calculation processes allowing the processes written in the MLlib libraries to be executed quickly allowing their use and above all at an industrial level.\\*

MLlib provides many types of algorithms, as well as numerous useful functions. ML includes classification algorithms, regression, decision trees, recommendation algorithms, grouping. Among the most used utilities can include the characteristics of transformations, standardization and normalization, statistical functions and linear algebra.

\subsection{Clustering}

Apache Spark follows a master/slave architecture with many main daemons and a cluster manager
\begin{itemize}

	\item Master Daemon — (Master/Driver Process)
	\item Worker Daemon –(Slave Process)
	\item Cluster Manager
\end{itemize}

As we can see in the figure~\ref{fig:clusters-spark} from \cite{brown:96} just
illustrates how figure can be included in the report.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{figs/cluster-overview.png}
	\caption{Cluster management of Apache Spark}
	\label{fig:clusters-spark}
\end{figure}

