\section{Conclusion}
\label{sec:conclusion}

Apache Spark has been designed in a way that it can scale up from one to thousands of computer nodes. 
Moreover, the simplicity of APIs in Spark and its processing speed makes Spark an excellent choice framework among data analysis projects. 

For the data exploration, Spark uses SQL shell. MLlib supports data analysis and Machine Learning. It lets handling the problem with large data size.

Hence Spark provides a simple way to parallelize the applications across Clusters and hides the complexity of distributed systems programming, network developer, and fault tolerance.