\section{Test-bed Environment and Experimental Results}
\label{sec:experiments}

About 3 pages that:

\begin{description}

\item[Describes] the software used to establish the test-bed and for implementing the demonstrator prototype.

\item[Explains] what experiments have been done and the results.

\end{description}

For some reports you may have to include a table with experimental
results are other kinds of tables that for instance compares
technologies. Table~\ref{tab:results} gives an example of how to create a table.

\begin{table}
\centering
\begin{tabular}{llrrrrrr}
  Config & Property & States & Edges & Peak & E-Time & C-Time & T-Time
  \\ \hline \hline
22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  485.7\% \\   
22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  471.4\% \\   
30-2 & B   &   14,672  &   41,611  &  4.9  \%  & 14 ms & 42.9\% &  464.3\% \\   
30-2 & C   &   14,672  &   41,611  &  4.9  \%  & 15 ms & 40.0\% &  420.0\% \\ \hline
10-3 & D   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 31.4\% &  285.7\% \\   
10-3 & E   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 34.3\% &  308.6\% \\
\hline \hline
\end{tabular}
\caption{Selected experimental results on the communication protocol example.}
\label{tab:results}
\end{table}

--------------------------------------------------------------------------------------------------------------------
\subsection{Jupyter Notebooks}

The software we used to experiment with the data was Jupyter Notebook to write python code and PySpark to run that python code into spark.
Jupyter Notebook (formerly IPython Notebook) is an open-source web application that lets you create and share documents containing live code, equations, visualizations, and narrative text. Also, it is a widely used application in the field of Data Science to create and share documents including data cleansing and transformation, numerical simulation, statistical modeling, data visualization, automatic learning and much more. It allows you to edit and run notebook documents through any web browser of your choice, and it can run on a local desktop that does not require Internet access, or it can be installed on a remote server and accessed through the Internet. We can also run Jupyter Notebook without any installation.

\subsection{PySpark}
PySpark is a python API for spark released by the Apache Spark community to support python with Spark. Using PySpark, one can easily integrate and work with RDD in python programming language too. Numerous features make PySpark such an amazing framework when it comes to working with huge datasets. Whether it is to perform computations on large data sets or to just analyze them, Data engineers are turning to this tool. Following are some of the said features.\\*

\noindent
The following features are the key features of PySpark:

\begin{description}
	\item [Real-time computations]: Because of the in-memory processing in PySpark framework, it shows low latency.
	\item [Polyglot]: PySpark framework is compatible with various languages like Scala, Java, Python, and R, which makes it one of the most preferable frameworks for processing huge datasets.
	\item [Caching and disk persistence]: PySpark framework provides powerful caching and very good disk persistence.
	\item [Fast processing]: PySpark framework is way faster than other traditional frameworks for big data processing.
	\item [Works well with RDD]: Python programming language is dynamically typed which helps when working with RDD.
\end{description}

\noindent
\subsection{About the dataset Taxi Problem}

The goal of this dataset is predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used. Our challenge is to do musing Machine Learning techniques.\\*

\subsection{Experiments }
We have done six experiments (Each experiment itâ€™s a model testing), three of them to try Linear regression, Decision tree regression and Random forest regression in Native Spark and the other three to try Linear regression, Decision tree regression and Random forest regression in Local Cluster mode.\\*


