\section{Test-bed Environment and Experimental Results}
\label{sec:experiments}

About 3 pages that:

\begin{description}

\item[Describes] the software used to establish the test-bed and for implementing the demonstrator prototype.

\item[Explains] what experiments have been done and the results.

\end{description}

For some reports you may have to include a table with experimental
results are other kinds of tables that for instance compares
technologies. Table~\ref{tab:results} gives an example of how to create a table.

\begin{table}
\centering
\begin{tabular}{llrrrrrr}
  Config & Property & States & Edges & Peak & E-Time & C-Time & T-Time
  \\ \hline \hline
22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  485.7\% \\   
22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  471.4\% \\   
30-2 & B   &   14,672  &   41,611  &  4.9  \%  & 14 ms & 42.9\% &  464.3\% \\   
30-2 & C   &   14,672  &   41,611  &  4.9  \%  & 15 ms & 40.0\% &  420.0\% \\ \hline
10-3 & D   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 31.4\% &  285.7\% \\   
10-3 & E   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 34.3\% &  308.6\% \\
\hline \hline
\end{tabular}
\caption{Selected experimental results on the communication protocol example.}
\label{tab:results}
\end{table}

--------------------------------------------------------------------------------------------------------------------
\subsection{Jupyter Notebooks}

The software we used to experiment with the data was Jupyter Notebook to write python code and PySpark to run that python code into spark.
Jupyter Notebook (formerly IPython Notebook) is an open-source web application that lets you create and share documents containing live code, equations, visualizations, and narrative text.\\*

Also, it is a widely used application in the field of Data Science to create and share documents including data cleansing and transformation, numerical simulation, statistical modeling, data visualization, automatic learning and much more.\\*
It allows you to edit and run notebook documents through any web browser of your choice, and it can run on a local desktop that does not require Internet access, or it can be installed on a remote server and accessed through the Internet. We can also run Jupyter Notebook without any installation.

\subsection{PySpark}
PySpark is a python API for spark released by the Apache Spark community to support python with Spark. Using PySpark, one can easily integrate and work with RDD in python programming language too. 

Numerous features make PySpark such an amazing framework when it comes to working with huge datasets. Whether it is to perform computations on large data sets or to just analyze them, Data engineers are turning to this tool. Following are some of the said features.\\*

\noindent
The following features are the key features of PySpark:

\begin{description}
	\item [Real-time computations]: Because of the in-memory processing in PySpark framework, it shows low latency.
	\item [Polyglot]: PySpark framework is compatible with various languages like Scala, Java, Python, and R, which makes it one of the most preferable frameworks for processing huge datasets.
	\item [Caching and disk persistence]: PySpark framework provides powerful caching and very good disk persistence.
	\item [Fast processing]: PySpark framework is way faster than other traditional frameworks for big data processing.
	\item [Works well with RDD]: Python programming language is dynamically typed which helps when working with RDD.
\end{description}

\noindent
\subsection{About the dataset Taxi Problem} \cite{taxiDataset}

The goal of this dataset is predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used. Our challenge is to do it using distributed Machine Learning techniques.\\*

We also choose this dataset because contains 6 Gb of data, so we think that is a proper amount of data for test our results.

\subsection{Experiments }

We have done six experiments (Each experiment it’s a model testing), to try Linear regression, Decision tree regression and Random forest regression in Native Clustering and the rest to try Linear regression, Decision tree regression and Random forest regression in Local Cluster mode.\\*

\begin{table}[h]
	\centering
	\begin{tabular}{llrrrrrr}
		Mode & Machine Learning Model & Time-1 & RMSE-1 & Time-2 & RMSE-2 & Time-3 & RMSE-3
		\\ \hline \hline
		Native Cluster  & Linear Regression   &    4:04 min  &   20.7  &  11:47 min & 20.7 & 8:17 min & 20.7 \\   
		Native Cluster  & Decision Tree   &    8:37 min  &   23.96  &  20:22 min  &  23.96  & 20:05 min &  23.96\\   
		Native Cluster  & Random Forest   &   11:50 min &   23.96  &  35:37 min  & 24.82  & 35:53 min & 24.82 \\  \hline
		Local Cluster   & Linear Regression   &   2:33 min &   20.7  &  7:40 min  & 20.7 & 7:36 min  & 20.7   \\ 
		Local Cluster   & Decision Tree   &   5:30 min &   16.49  & 11:35 min & 24.03 & 13:05 min & 24.03  \\   
		Local Cluster   & Random Forest   &   8:44 min &   23.96  & 18:30 min & 23.96 & \textbf{26:28 min} & \textbf{6.86}  \\
		\hline \hline
	\end{tabular}
	\caption{Selected experimental results on the training models.}
	\label{tab:results}
\end{table}

We have two important approaches, one is that Linear Regression is the fastest model, and Random Forest is the worst (What it isn’t so important for our main purpose), and the other one is that clearly, with local clustering the machine invest much less time than with Native Spark training the same models respectively.
Also, it’s better to use Clustering with several machines, but we couldn’t really experiment with that.
